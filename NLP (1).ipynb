{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOJoApJkiqMj"
      },
      "source": [
        "# Project 3\n",
        "Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo44OsBohUYh"
      },
      "source": [
        "Make a program that can handle tokenization rules specified by regular expressions and exception lists (abbreviations, acronyms, etc.). Then complete the project by writing the necessary rules for proper tokenization of a language other than English considering also emoticons and emojis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdhsy4kbhWcR"
      },
      "source": [
        "Here is a Python program that can handle tokenization rules specified by regular expressions and exception lists (abbreviations, acronyms, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8B6A8DBkhZB7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, regex_rules, exception_lists):\n",
        "        self.regex_rules = regex_rules\n",
        "        self.exception_lists = exception_lists\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        if type(text) == type(\"\"):\n",
        "            text = [text]\n",
        "        text = self._apply_exception_lists(text)\n",
        "        tokens = self._apply_regex_rules(text)\n",
        "        return tokens\n",
        "    \n",
        "    def _apply_exception_lists(self, text):\n",
        "        for exception_list in self.exception_lists:\n",
        "            for exception in exception_list:\n",
        "                for n in range(len(text)):\n",
        "                    idx = n\n",
        "                    if exception in text[n] and text[n] != exception.strip():\n",
        "                        sublist = text[n].split(exception)\n",
        "                        if text[n].endswith(exception):\n",
        "                            a_list = [sublist[0].strip(), exception.strip()]\n",
        "                        elif text[n].startswith(exception):\n",
        "                            a_list = [exception.strip(), sublist[1].strip()]\n",
        "                        else:\n",
        "                            a_list = [sublist[0].strip(), exception.strip(), sublist[1].strip()]\n",
        "                        del text[idx]\n",
        "                        for el in a_list:\n",
        "                            text.insert(idx, el)\n",
        "                            idx +=1\n",
        "                            \n",
        "        return text\n",
        "\n",
        "    \n",
        "    def _apply_regex_rules(self, text):\n",
        "        for regex_rule in self.regex_rules:\n",
        "            second_text = []\n",
        "            for el in text:\n",
        "                second_text.append(el)\n",
        "            idx = 0\n",
        "            for substring in second_text:\n",
        "                new_subtext = \"\"\n",
        "                new_subtext_list = []\n",
        "                if self._check_exceptions(substring):\n",
        "                    new_subtext = re.sub(regex_rule, r' \\1 ', substring)\n",
        "                    new_subtext_list = new_subtext.split()\n",
        "                    del text[idx]\n",
        "                    for el in new_subtext_list:\n",
        "                        text.insert(idx, el)\n",
        "                        idx +=1\n",
        "                else:\n",
        "                    idx +=1\n",
        "                \n",
        "                            \n",
        "        return text\n",
        "    \n",
        "    def _check_exceptions(self, substring):\n",
        "        for exception_list in self.exception_lists:\n",
        "            for exception in exception_list:\n",
        "                if exception not in substring:\n",
        "                    value = True\n",
        "                elif exception in substring:\n",
        "                    return False\n",
        "        return value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6PlVfnhduA"
      },
      "source": [
        "To tokenize a language other than English, you would need to provide the appropriate regular expression rules and exception lists. Here is an example of tokenization rules for Spanish:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGSvOmG7heup",
        "outputId": "ca8ad3af-a8d3-430a-90c0-785671a86bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Questo', 'è', 'un', 'test', 'di', 'tokenizzazione', ',', 'e', 'vediamo', 'come', 'va', ',', 'perché', 'ci', 'serve', 'caro', 'dott.', \"Dell'Anda\", '.', 'Io', 'credevo', 'che', 'la', 'batteria', \"dell'automobile\", 'fosse', 'ok', ';', 'ma', 'in', 'realtà', 'non', 'lo', 'è', '.', 'Le', 'lascio', 'il', 'mio', 'email', 'f', '.', 'tambu@unibo', '.', 'it', 'e', 'la', 'mia', 'URL', 'http', ':', '//www', '.', 'unibo', '.', 'it', 'così', 'controlla', '.', 'Io', 'sono', \"l'\", 'Ing.', 'Geom.', 'geom.', 'Avv.', 'prof.', 'dr.', 'Pincopallo', 'e', 'andrò', 'negli', 'U.S.A.', 'e', 'vediamo', 'se', 'mi', 'compri', 'una', 'S', '.', 'r', '.', 'l', '.', 'e', 'poi', 'la', 'mettiamo', 'alla', 'c', '.', 'a', '.', 'di', 'Luigi', 'come', 'p', '.', 'es', '.', 'la', 'racc.', 'P', '.', 'S', '.', ':', 'Nuovo', 'paragrafo', 'EVVIVA', '!', 'L’apprezzerò', 'di', 'più', '!', '\"Ma', 'le', 'quote', 'come', 'le', 'gestisce', '?', '\"', \"'Ma\", 'le', 'quote', \"'come'\", 'le', 'gestisce', '?', \"'\", \"dell'automobile\", \"c'è\", 'c’è', \"e'\", '‘e’', \"citta'\", 'città’', 'citta’', \"l'Italia\", 'l’Italia', \"all'estero\", 'all’estero', \"l'hanno\", 'l’hanno', \"'città'\", 'aperta', '‘città’', 'aperta', '#Grisù', 'presidente', '!', '10', '.', '9', '100', ',', '000', ',', '000', '10', ',', '9', '100', '.', '000', '.', '000', '10', '.', '67', '%', '10', ',', '67', '%', '10', ',', '56', ',', '89', '10', '.', '56', '.', '89', \"‘anch'io’\", '‘anch’io’', 'mi', 'stai', 'proprio', \"'sui\", \"coglioni'\", '!', 'mi', '“stai”', 'proprio', \"'sui\", \"coglioni'\", 'nato', 'nel', \"'\", '73', ',', 'bello', '!', \"l'\", '84', '%', 'di', 'noi', 'un', \"'bel'\", '22', '%', 'un', \"'bell'uomo\", 'E', \"c'è\", 'pure', 'gli', 'emoticon', '!', '!', '!', ':', 'P', ':', '-', ')', ';)', 'Viva', 'viva', '<3', 'il', 'regimentoooo', '!', 'un', \"'bell’uomo\"]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Regular expression rules\n",
        "regex_rules = [\n",
        "    r'([,.;:!¡?()¿])',\n",
        "    r'(\\d+)',\n",
        "    r'(:[)(pP])',\n",
        "    r'([a-z][.][a-z][.])'\n",
        "]\n",
        "\n",
        "# Exception lists\n",
        "exception_lists = [\n",
        "    # Abbreviations\n",
        "    ['Ing.', 'Geom.', 'geom.', 'Avv.', 'prof.', 'dr.', 'dott.','racc.'],\n",
        "    # Acronyms\n",
        "    ['U.S.A.', 'U.E.'],\n",
        "    # Emoticons and emojis\n",
        "    [':)', ':(', ';)', ':D', '<3']\n",
        "]\n",
        "\n",
        "# Create tokenizer object\n",
        "tokenizer = Tokenizer(regex_rules, exception_lists)\n",
        "\n",
        "# Tokenize text\n",
        "text = \"\"\"Questo è un test di tokenizzazione, e vediamo come va, perché ci serve \n",
        "caro dott. Dell'Anda. Io credevo che la batteria dell'automobile fosse \n",
        "ok; ma in realtà non lo è.\n",
        "Le lascio il mio email f.tambu@unibo.it e la mia URL http://www.unibo.it \n",
        "così controlla.\n",
        "Io sono l'Ing. Geom. geom. Avv. prof. dr. Pincopallo e andrò negli \n",
        "U.S.A. e vediamo se mi compri una S.r.l. e poi la mettiamo\n",
        "alla c.a. di Luigi come p.es. la racc.\n",
        "\n",
        "P.S.:\n",
        "        Nuovo paragrafo EVVIVA! L’apprezzerò di più!\n",
        "\"Ma le quote come le gestisce?\"\n",
        "'Ma le quote 'come' le gestisce?'\n",
        "\n",
        "dell'automobile\n",
        "c'è c’è\n",
        "e'\n",
        "‘e’\n",
        "citta' città’ citta’\n",
        "l'Italia l’Italia\n",
        "all'estero all’estero\n",
        "l'hanno l’hanno\n",
        "'città' aperta ‘città’ aperta\n",
        "#Grisù presidente!\n",
        "10.9 100,000,000\n",
        "10,9 100.000.000\n",
        "10.67% 10,67%\n",
        "10,56,89 10.56.89\n",
        "‘anch'io’ ‘anch’io’\n",
        "mi stai proprio 'sui coglioni'!\n",
        "mi “stai” proprio 'sui coglioni'\n",
        "nato nel '73, bello!\n",
        "l'84% di noi\n",
        "un 'bel'22%\n",
        "un 'bell'uomo\n",
        "E c'è pure gli emoticon!!! :P\n",
        ":-) ;) Viva viva <3 il regimentoooo!\n",
        "un 'bell’uomo\n",
        "\"\"\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSfJq6dG8ZHv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7F7-xvz8ZHv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}