{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOJoApJkiqMj"
   },
   "source": [
    "# Project\n",
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8B6A8DBkhZB7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, regex_rules, exception_lists):\n",
    "        self.regex_rules = regex_rules\n",
    "        self.exception_regex = exception_regex\n",
    "        self.exception_lists = exception_lists\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        if type(text) == type(\"\"):\n",
    "            text = [text]\n",
    "        text = self._apply_exception_regex(text)\n",
    "        text = self._apply_exception_lists(text)\n",
    "        tokens = self._apply_regex_rules(text)\n",
    "        return tokens\n",
    "    \n",
    "    def _apply_exception_regex(self, text):\n",
    "        for subtext in text:\n",
    "            for regex in self.exception_regex:\n",
    "                a_list = re.findall(regex, subtext)\n",
    "                for el in a_list: \n",
    "                    self.exception_lists[-1].append(el)\n",
    "        return text\n",
    "    \n",
    "    def _apply_exception_lists(self, text):\n",
    "        for exception_list in self.exception_lists:\n",
    "            for exception in exception_list:\n",
    "                for n in range(len(text)):\n",
    "                    idx = n\n",
    "                    if exception in text[n] and text[n] != exception.strip():\n",
    "                        sublist = text[n].split(exception)\n",
    "                        for item in sublist:\n",
    "                            if text[n].startswith(exception):\n",
    "                                a_list = [exception]\n",
    "                            else:\n",
    "                                a_list = []\n",
    "                            for el in sublist:\n",
    "                                a_list.append(el)\n",
    "                                if el != sublist[-1]:\n",
    "                                    a_list.append(exception)\n",
    "                            if text[n].endswith(exception):\n",
    "                                a_list.append(exception)                            \n",
    "                        del text[idx]\n",
    "                        for el in a_list:\n",
    "                            text.insert(idx, el)\n",
    "                            idx +=1\n",
    "                    elif exception in text[n] and text[n] == exception.strip():\n",
    "                        idx += 1              \n",
    "        return text\n",
    "\n",
    "    \n",
    "    def _apply_regex_rules(self, text):\n",
    "        for regex_rule in self.regex_rules:\n",
    "            second_text = []\n",
    "            for el in text:\n",
    "                second_text.append(el)\n",
    "            idx = 0\n",
    "            for substring in second_text:\n",
    "                new_subtext = \"\"\n",
    "                new_subtext_list = []\n",
    "                if self._check_exceptions(substring):\n",
    "                    new_subtext = re.sub(regex_rule, r' \\1 ', substring)\n",
    "                    new_subtext_list = new_subtext.split()\n",
    "                    del text[idx]\n",
    "                    for el in new_subtext_list:\n",
    "                        text.insert(idx, el)\n",
    "                        idx +=1\n",
    "                else:\n",
    "                    idx +=1\n",
    "                \n",
    "                            \n",
    "        return text\n",
    "    \n",
    "    def _check_exceptions(self, substring):\n",
    "        for exception_list in self.exception_lists:\n",
    "            for exception in exception_list:\n",
    "                if exception not in substring:\n",
    "                    value = True\n",
    "                elif exception in substring:\n",
    "                    return False\n",
    "        return value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6PlVfnhduA"
   },
   "source": [
    "To tokenize a language other than English, you would need to provide the appropriate regular expression rules and exception lists. Here is an example of tokenization rules for Spanish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGSvOmG7heup",
    "outputId": "d3d816f1-6df8-4fd3-a7ed-9e39e4279eaa",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.003758192062378 seconds\n",
      "['Questo', 'è', 'un', 'test', 'di', 'tokenizzazione', ',', 'e', 'vediamo', 'come', 'va', ',', 'perché', 'ci', 'serve', 'caro', 'dott.', \"Dell'Anda\", '.', 'Io', 'credevo', 'che', 'la', 'batteria', \"Dell'\", 'automobile', 'fosse', 'ok', ';', 'ma', 'in', 'realtà', 'non', 'lo', 'è', '.', 'Le', 'lascio', 'il', 'mio', 'email', 'f.tambu@unibo.it', 'e', 'la', 'mia', 'URL', 'http://www.unibo.it', 'così', 'controlla', '.', 'Io', 'sono', \"l'\", 'Ing.', 'Geom.', 'geom.', 'Avv.', 'prof.', 'dr.', 'Pincopallo', 'e', 'andrò', 'negli', 'U.S.A.', 'e', 'vediamo', 'se', 'mi', 'compri', 'una', 'S.r.l', '.', 'e', 'poi', 'la', 'mettiamo', 'alla', 'c.a.', 'di', 'Luigi', 'come', 'p.es', '.', 'la', 'racc.', 'P.S.', ':', 'Nuovo', 'paragrafo', 'EVVIVA', '!', 'L’', 'apprezzerò', 'di', 'più', '!', '\"Ma', 'le', 'quote', 'come', 'le', 'gestisce?', '\"', \"'Ma\", 'le', 'quote', \"'come\", \"'\", 'le', 'gestisce?', \"'\", \"dell'\", 'automobile', \"c'\", 'è', 'c’', 'è', 'e', \"'\", '‘', 'e’', 'citta', \"'\", 'città’', 'citta’', \"l'\", 'Italia', 'l’', 'Italia', \"all'\", 'estero', 'all’', 'estero', \"l'\", 'hanno', 'l’', 'hanno', \"'città\", \"'\", 'aperta', '‘città’', 'aperta', '#Grisù', 'presidente', '!', '10.9', '100,000,000', '10,9', '100.000.000', '10.67%', '10,67%', '10,56,89', '10.56.89', '‘', \"anch'\", 'io’', '‘', 'anch’', 'io’', 'mi', 'stai', 'proprio', \"'sui\", \"coglioni'\", '!', 'mi', '“stai”', 'proprio', \"'sui\", 'coglioni', \"'\", 'nato', 'nel', \"'73\", ',', 'bello', '!', \"l'\", '84%', 'di', 'noi', 'un', \"'\", \"bel'\", '22%', 'un', \"'\", \"bell'\", 'uomo', 'E', \"c'\", 'è', 'pure', 'gli', 'emoticon!!', '!', ':P', ':-)', ';)', 'Viva', 'viva', '<3', 'il', 'regimentoooo', '!', 'un', \"'\", 'bell’', 'uomo']\n"
     ]
    }
   ],
   "source": [
    "# Regular expression rules\n",
    "regex_rules = [\n",
    "    r\"\"\"([.,;:!'\"?()]\\s)\"\"\", #lasciando il punto e la virgola qui, i decimali e le mail non vengono tokenizzati.\n",
    "    r\"([a-zA-Z]+[’\\\\']{1})\", #articoli/preposizioni articolate con apostrofo, elisioni -> \"Dell'Anda\"viene separato\n",
    "    r'([a-z][.][a-z][.])', \n",
    "]\n",
    "\n",
    "# Exception lists\n",
    "exception_lists = [\n",
    "    # Abbreviations\n",
    "    ['Ing.', 'Geom.', 'geom.', 'Avv.', 'prof.', 'dr.', 'dott.','racc.', 'c.a.'],\n",
    "    # Acronyms\n",
    "    ['U.S.A.', 'U.E.'],\n",
    "    # Emoticons and emojis\n",
    "    [':)', ':(', ';)', ':D', '<3',':P',':-)'],\n",
    "    # Empty list\n",
    "    []\n",
    "]\n",
    "\n",
    "exception_regex = [\n",
    "    r\"([dD][a-z]+[’\\\\']{1}[A-Z]{1}[a-z]+)\", #preposizioni articolate nei cognomi (es.: Dell'Anda)\n",
    "    r'([\\d]+[,\\\\.]+[\\d\\\\.,]+[%]?)', #numeri decimali (. e ,) e percentuali\n",
    "    r'(http://www.[\\w/]+.\\w+)', #url (anche con percorsi interni del tipo /.../.../....it)\n",
    "    r'([a-z]+[\\\\.][a-z]+@{1}[a-z]+[\\\\.]+[a-z]+)' #email\n",
    "]\n",
    "\n",
    "# Create tokenizer object\n",
    "tokenizer = Tokenizer(regex_rules, exception_lists)\n",
    "\n",
    "\n",
    "text = \"\"\"Questo è un test di tokenizzazione, e vediamo come va, perché ci serve \n",
    "caro dott. Dell'Anda. Io credevo che la batteria Dell'automobile fosse \n",
    "ok; ma in realtà non lo è.\n",
    "Le lascio il mio email f.tambu@unibo.it e la mia URL http://www.unibo.it \n",
    "così controlla.\n",
    "Io sono l'Ing. Geom. geom. Avv. prof. dr. Pincopallo e andrò negli \n",
    "U.S.A. e vediamo se mi compri una S.r.l. e poi la mettiamo\n",
    "alla c.a. di Luigi come p.es. la racc.\n",
    "\n",
    "P.S.:\n",
    "        Nuovo paragrafo EVVIVA! L’apprezzerò di più!\n",
    "\"Ma le quote come le gestisce?\"\n",
    "'Ma le quote 'come' le gestisce?'\n",
    "\n",
    "dell'automobile\n",
    "c'è c’è\n",
    "e'\n",
    "‘e’\n",
    "citta' città’ citta’\n",
    "l'Italia l’Italia\n",
    "all'estero all’estero\n",
    "l'hanno l’hanno\n",
    "'città' aperta ‘città’ aperta\n",
    "#Grisù presidente!\n",
    "10.9 100,000,000\n",
    "10,9 100.000.000\n",
    "10.67% 10,67%\n",
    "10,56,89 10.56.89\n",
    "‘anch'io’ ‘anch’io’\n",
    "mi stai proprio 'sui coglioni'!\n",
    "mi “stai” proprio 'sui coglioni'\n",
    "nato nel '73, bello!\n",
    "l'84% di noi\n",
    "un 'bel'22%\n",
    "un 'bell'uomo\n",
    "E c'è pure gli emoticon!!! :P\n",
    ":-) ;) Viva viva <3 il regimentoooo!\n",
    "un 'bell’uomo\n",
    "\"\"\"\n",
    "# Tokenize text\n",
    "\n",
    "st = time.time()\n",
    "time.sleep(1)\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
